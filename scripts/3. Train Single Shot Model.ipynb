{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSD 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from functools import partial\n",
    "\n",
    "Conv2D = partial(Conv2D, kernel_size=(3,3), \n",
    "                 activation='relu', padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 + 1 # 0~9 + Background\n",
    "num_priors = 5\n",
    "num_units = 16\n",
    "\n",
    "inputs = Input(shape=(None,None,3)) \n",
    "\n",
    "# BLOCK 1\n",
    "conv1_1 = Conv2D(num_units, name='conv1_1')(inputs)\n",
    "norm1_1 = BatchNormalization(name='norm1_1')(conv1_1)\n",
    "conv1_2 = Conv2D(num_units, name='conv1_2')(norm1_1)\n",
    "norm1_2 = BatchNormalization(name='norm1_2')(conv1_2)\n",
    "conv1_3 = Conv2D(num_units, name='conv1_3')(norm1_2)\n",
    "norm1_3 = BatchNormalization(name='norm1_3')(conv1_3)\n",
    "\n",
    "# BLOCK 2\n",
    "conv2_1 = Conv2D(num_units * 2, name='conv2_1')(norm1_3)\n",
    "norm2_1 = BatchNormalization(name='norm2_1')(conv2_1)\n",
    "conv2_2 = Conv2D(num_units * 2, strides=(2,2),name='conv2_2')(norm2_1)\n",
    "norm2_2 = BatchNormalization(name='norm2_2')(conv2_2)\n",
    "\n",
    "# BLOCK 3\n",
    "conv3_1 = Conv2D(num_units * 4, name='conv3_1')(norm2_2)\n",
    "norm3_1 = BatchNormalization(name='norm3_1')(conv3_1)\n",
    "conv3_2 = Conv2D(num_units * 4, strides=(2,2), name='conv3_2')(norm3_1)\n",
    "norm3_2 = BatchNormalization(name='norm3_2')(conv3_2)\n",
    "\n",
    "# BLOCK 4\n",
    "conv4_1 = Conv2D(num_units * 8, name='conv4_1')(norm3_2)\n",
    "norm4_1 = BatchNormalization(name='norm4_1')(conv4_1)\n",
    "conv4_2 = Conv2D(num_units * 8, strides=(2,2), name='conv4_2')(norm4_1)\n",
    "norm4_2 = BatchNormalization(name='norm4_2')(conv4_2)\n",
    "\n",
    "# Block 5\n",
    "conv5_1 = Conv2D(num_units * 8, name='conv5_1')(norm4_2)\n",
    "norm5_1 = BatchNormalization(name='norm5_1')(conv5_1)\n",
    "conv5_2 = Conv2D(num_units * 8, strides=(2,2), name='conv5_2')(norm5_1)\n",
    "norm5_2 = BatchNormalization(name='norm5_2')(conv5_2)\n",
    "\n",
    "\n",
    "heads = []\n",
    "source_layers = [norm3_2, norm4_2, norm5_2]\n",
    "for idx, source_layer in enumerate(source_layers):\n",
    "    # Classification\n",
    "    clf = Conv2D(num_priors * num_classes, activation=None,\n",
    "                 name=f'clf_head{idx}_logit')(source_layer)\n",
    "    clf = Reshape((-1, num_classes),\n",
    "                  name=f'clf_head{idx}_reshape')(clf)\n",
    "    clf = Softmax(axis=-1, name=f'clf_head{idx}')(clf)\n",
    "\n",
    "    # Localization\n",
    "    loc = Conv2D(num_priors * 4, activation=None,\n",
    "                 name=f'loc_head{idx}')(source_layer)\n",
    "    loc = Reshape((-1,4),\n",
    "                  name=f'loc_head{idx}_reshape')(loc)\n",
    "    \n",
    "    \n",
    "    head = Concatenate(axis=-1, name=f'head{idx}')([clf, loc])\n",
    "    heads.append(head)\n",
    "    \n",
    "predictions = Concatenate(axis=1, name='predictions')(heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "model = Model(inputs, predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anchors:\n",
    "    \"\"\"\n",
    "    Anchor Configuration Class\n",
    "    \"\"\"\n",
    "    bbox_df = pd.DataFrame()\n",
    "\n",
    "    def __init__(self, strides, scales, ratios):\n",
    "        self.strides = strides\n",
    "        self.scales = scales\n",
    "        self.ratios = ratios\n",
    "        self.setup()\n",
    "\n",
    "    def generate(self, image_shape):\n",
    "        \"\"\"\n",
    "        image_shape에 맞춰서, Anchor(==Default Boxes)를 구성\n",
    "\n",
    "        return :\n",
    "        (# Anchors, 4)로 이루어진 출력값 생성\n",
    "        \"\"\"\n",
    "        height, width = image_shape[:2]\n",
    "        multi_boxes = []\n",
    "        for stride, df in self.bbox_df.groupby('stride'):\n",
    "            boxes = []\n",
    "            for idx, row in df.iterrows():\n",
    "                stride, box_width, box_height = row.stride, row.w, row.h\n",
    "                ys, xs = np.mgrid[0:height:stride, 0:width:stride]\n",
    "                box_width = np.ones_like(xs) * box_width\n",
    "                box_height = np.ones_like(ys) * box_height\n",
    "                center_xs = stride // 2 + xs\n",
    "                center_ys = stride // 2 + ys\n",
    "\n",
    "                block_centers = np.stack((center_xs, center_ys,\n",
    "                                          box_width, box_height),\n",
    "                                         axis=-1)\n",
    "                boxes.append(block_centers)\n",
    "            boxes = np.stack(boxes, axis=2)\n",
    "            boxes = np.reshape(boxes, (-1, 4))\n",
    "            multi_boxes.append(boxes)\n",
    "        multi_boxes = np.concatenate(multi_boxes, axis=0)\n",
    "        return multi_boxes\n",
    "\n",
    "    def setup(self):\n",
    "        bbox_df = pd.DataFrame(columns=['stride', 'w', 'h'])\n",
    "        for scale, stride in zip(self.scales, self.strides):\n",
    "            for ratio in self.ratios:\n",
    "                w = np.round(scale * ratio[0]).astype(np.int)\n",
    "                h = np.round(scale * ratio[1]).astype(np.int)\n",
    "                bbox_df.loc[len(bbox_df) + 1] = [stride, w, h]\n",
    "\n",
    "        bbox_df.stride = bbox_df.stride.astype(np.int)\n",
    "        bbox_df.w = bbox_df.w.astype(np.int)\n",
    "        bbox_df.h = bbox_df.h.astype(np.int)\n",
    "        self.bbox_df = bbox_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strides = [4, 8, 16]\n",
    "scales = [10, 20, 30]\n",
    "ratios = [(1,1),     # ratio : 1.\n",
    "          (0.5,1.5), # ratio : 0.33\n",
    "          (0.8,1.2), # ratio : 0.67\n",
    "          (1.2,0.8), # ratio : 1.5\n",
    "          (1.4,1.4)  # ratio : 1\n",
    "         ] \n",
    "\n",
    "anchors = Anchors(strides, scales, ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습하기\n",
    "\n",
    "우리는 학습할 모델과 데이터를 이전 시간에서 꾸렸습니다. 이제 학습까지 남은 것은 모델을 어떻게 학습시킬까?입니다. 이 중 제일 핵심은 바로 Loss함수 설계에 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 구성하기\n",
    "\n",
    "우리가 학습해야 하는 것은 위치를 추론하는 Regressor와 사물을 분류하는 Classifer입니다. Regressor의 경우에는 SmoothL1이라 불리는 Loss로 학습을 시키고, Classifier은 분류모델에서 주로 사용하는 Cross-Entropy Loss를 이용합니다. Regressor의 경우에는 당연하게도 Matched prior box의 경우 한에서만 학습을 해야 합니다. SmoothL1은 MAE와 MSE의 합쳐놓은 형태로, 수식은 아래와 같습니다.\n",
    "\n",
    "$\n",
    "smooth_{L1}(x) = \\begin{cases}\n",
    "0.5x^2, \\mbox{  if  } |x| <1\\\\\n",
    "|x| - 0.5 \\mbox{   otherwise,}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "Confidence Loss를 계산할 때, 중요한 문제가 하나 있습니다. 바로 Class Imbalance 문제입니다. 영상에서 대부분은 BackGround에 해당합니다. 우리가 원하는 Foreground에 매칭된 Prior box는 극히 일부분에 불과합니다. 이 때문에, Easy Negative Sample, 즉 Background에 대한 Loss가 지나치게 커서, 실제로 학습하고자 하는 Foreground에 대한 학습은 잘 이루어지지 않게 됩니다. 이를 방지하기 위해, Negative Sample 중 가장 Loss가 큰것들을 위주로만 추출하여 학습하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSDLoss(alpha=1., pos_neg_ratio=3.):\n",
    "    def ssd_loss(y_true, y_pred):\n",
    "        num_classes = tf.shape(y_true)[2] - 4\n",
    "        y_true = tf.reshape(y_true, [-1, num_classes + 4])\n",
    "        y_pred = tf.reshape(y_pred, [-1, num_classes + 4])\n",
    "        eps = K.epsilon()\n",
    "\n",
    "        # Split Classification and Localization output\n",
    "        y_true_clf, y_true_loc = tf.split(y_true, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "        y_pred_clf, y_pred_loc = tf.split(y_pred, \n",
    "                                          [num_classes, 4], \n",
    "                                          axis=-1)\n",
    "\n",
    "        # split foreground & background\n",
    "        neg_mask = y_true_clf[:, -1]\n",
    "        pos_mask = 1 - neg_mask\n",
    "        num_pos = tf.reduce_sum(pos_mask)\n",
    "        num_neg = tf.reduce_sum(neg_mask)\n",
    "        num_neg = tf.minimum(pos_neg_ratio * num_pos, num_neg)\n",
    "\n",
    "        # softmax loss\n",
    "        y_pred_clf = K.clip(y_pred_clf, eps, 1. - eps)\n",
    "        clf_loss = -tf.reduce_sum(y_true_clf * tf.math.log(y_pred_clf),\n",
    "                                  axis=-1)\n",
    "        pos_clf_loss = tf.reduce_sum(clf_loss * pos_mask) / (num_pos + eps)\n",
    "        neg_clf_loss = clf_loss * neg_mask\n",
    "        values, indices = tf.nn.top_k(neg_clf_loss,\n",
    "                                      k=tf.cast(num_neg, tf.int32))\n",
    "        neg_clf_loss = tf.reduce_sum(values) / (num_neg + eps)\n",
    "        clf_loss = pos_clf_loss + neg_clf_loss\n",
    "        \n",
    "        # smooth l1 loss\n",
    "        l1_loss = tf.math.abs(y_true_loc - y_pred_loc)\n",
    "        l2_loss = 0.5 * (y_true_loc - y_pred_loc) ** 2\n",
    "        loc_loss = tf.where(tf.less(l1_loss, 1.0),\n",
    "                            l2_loss,\n",
    "                            l1_loss - 0.5)\n",
    "        loc_loss = tf.reduce_sum(loc_loss, axis=-1)\n",
    "        loc_loss = tf.reduce_sum(loc_loss * pos_mask) / (num_pos + eps)\n",
    "\n",
    "        # total loss\n",
    "        return clf_loss + alpha * loc_loss\n",
    "    return ssd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 구성하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - IOU 계산하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(gt_boxes, pr_boxes):\n",
    "    # 1. pivot bounding boxes \n",
    "    exp_gt_boxes = gt_boxes[:,None] # Ground truth box가 행 기준으로 정렬되도록\n",
    "    exp_pr_boxes = pr_boxes[None,:] # prior box가 열 기준으로 정렬되도록\n",
    "\n",
    "    # 2. calculate intersection over union\n",
    "    # 2.1. Calculate Intersection\n",
    "    gt_cx, gt_cy, gt_w, gt_h = exp_gt_boxes.transpose(2,0,1)\n",
    "    pr_cx, pr_cy, pr_w, pr_h = exp_pr_boxes.transpose(2,0,1)\n",
    "\n",
    "    # (cx,cy,w,h) -> (xmin,ymin,xmax,ymax)\n",
    "    gt_xmin, gt_xmax = gt_cx-gt_w/2, gt_cx+gt_w/2\n",
    "    gt_ymin, gt_ymax = gt_cy-gt_h/2, gt_cy+gt_h/2\n",
    "    pr_xmin, pr_xmax = pr_cx-pr_w/2, pr_cx+pr_w/2\n",
    "    pr_ymin, pr_ymax = pr_cy-pr_h/2, pr_cy+pr_h/2\n",
    "\n",
    "    # 겹친 사각형의 너비와 높이 구하기\n",
    "    in_xmin = np.maximum(gt_xmin, pr_xmin)\n",
    "    in_xmax = np.minimum(gt_xmax, pr_xmax)\n",
    "    in_width = np.maximum(0,in_xmax - in_xmin)\n",
    "\n",
    "    in_ymin = np.maximum(gt_ymin, pr_ymin)\n",
    "    in_ymax = np.minimum(gt_ymax, pr_ymax)\n",
    "    in_height = np.maximum(0,in_ymax - in_ymin)\n",
    "\n",
    "    # 겹친 사각형의 넓이 구하기\n",
    "    intersection = in_width*in_height\n",
    "\n",
    "    gt_sizes = exp_gt_boxes[...,2] * exp_gt_boxes[...,3]\n",
    "    pr_sizes = exp_pr_boxes[...,2] * exp_pr_boxes[...,3]\n",
    "\n",
    "    # 2.2. Calculate Union\n",
    "    union = (gt_sizes + pr_sizes) - intersection\n",
    "\n",
    "    # 0 나누기 방지를 위함\n",
    "    return (intersection / (union+1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Classification Network의 정답 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def convert_classification_gt_to_model_form(\n",
    "    gt_labels, pr_boxes, match_indices):\n",
    "    num_classes = 10\n",
    "    num_anchors = len(pr_boxes)\n",
    "    gt_match_indices = match_indices[:, 0]\n",
    "    pr_match_indices = match_indices[:, 1]\n",
    "        \n",
    "    y_true_clf = np.full((num_anchors,), num_classes)\n",
    "    y_true_clf[pr_match_indices] = gt_labels[gt_match_indices]\n",
    "    return to_categorical(y_true_clf, num_classes=num_classes+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Localization Network의 정답 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_localization_gt_to_model_form(\n",
    "    gt_boxes, pr_boxes, match_indices):\n",
    "    num_anchors = len(pr_boxes)\n",
    "    gt_match_indices = match_indices[:, 0]\n",
    "    pr_match_indices = match_indices[:, 1]\n",
    "        \n",
    "    y_true_loc = np.zeros((num_anchors, 4))\n",
    "    g_cx, g_cy, g_w, g_h = gt_boxes[gt_match_indices].transpose()\n",
    "    p_cx, p_cy, p_w, p_h = pr_boxes[pr_match_indices].transpose()\n",
    "\n",
    "    hat_g_cx = (g_cx - p_cx) / p_w\n",
    "    hat_g_cy = (g_cy - p_cy) / p_h\n",
    "    hat_g_w = np.log(g_w / p_w)\n",
    "    hat_g_h = np.log(g_h / p_h)\n",
    "\n",
    "    hat_g = np.stack([hat_g_cx,hat_g_cy,hat_g_w,hat_g_h],axis=1)\n",
    "    y_true_loc[pr_match_indices] = hat_g\n",
    "    return y_true_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = get_file(\"mnist_detection.npz\",\n",
    "\"https://pai-datasets.s3.ap-northeast-2.amazonaws.com/alai-deeplearning/mnist_detection.npz\")\n",
    "data = np.load(fpath)\n",
    "\n",
    "train_images = data['train_images']\n",
    "train_labels = data['train_labels']\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 입력값 정규화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.~ 1. 으로 값의 범위 정규화하기\n",
    "train_X = train_images / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 라벨 폼 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 10000\n",
    "iou_threshold = 0.5\n",
    "\n",
    "image_shape = train_images.shape[1:]\n",
    "pr_boxes = anchors.generate(image_shape)\n",
    "num_anchors = len(pr_boxes)\n",
    "\n",
    "train_Y = np.zeros((num_data, num_anchors, 15))\n",
    "for i in tqdm(range(num_data)):\n",
    "    gt_boxes = train_labels[train_labels[:,0]==i,1:5]\n",
    "    gt_labels = train_labels[train_labels[:,0]==i,-1]\n",
    "    \n",
    "    iou = calculate_iou(gt_boxes, pr_boxes)\n",
    "    match_indices = np.argwhere(iou>=iou_threshold)\n",
    "    \n",
    "    y_true_clf = convert_classification_gt_to_model_form(\n",
    "        gt_labels, pr_boxes, match_indices)\n",
    "    y_true_loc = convert_localization_gt_to_model_form(\n",
    "        gt_boxes, pr_boxes, match_indices)    \n",
    "    \n",
    "    y_true = np.concatenate([y_true_clf, y_true_loc],axis=-1)\n",
    "    \n",
    "    train_Y[i] = y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model.compile(Adam(1e-3),\n",
    "              loss=SSDLoss(1.0,3.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_X, train_Y, \n",
    "          validation_split=0.1,\n",
    "          batch_size=64, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
